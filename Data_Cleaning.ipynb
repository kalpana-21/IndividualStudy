{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcxCORQ9iuX9WPKH/btngj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalpana-21/IndividualStudy/blob/main/Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSYsGzQ_w8jC",
        "outputId": "7f0b0d31-0e8a-4713-df51-0cd4d61d1daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50Ve31xmzeCv",
        "outputId": "17cbfdd1-da65-4d72-de39-b22a74f8a95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F0oKL3zzhZ8",
        "outputId": "a76d5e36-87aa-4c4c-eb2e-673bf848458d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "#for the tokenization step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewcMtZVtzueZ",
        "outputId": "03f27f1f-27ab-4959-e16a-2cbbbfb7fc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "#for lemmatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK1VF82jz6Eo",
        "outputId": "07c776e9-4830-4a27-a14a-9fa2aa76e54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "8W4MyySOFc5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def process_file(input_file_path, output_file_path):\n",
        "    # Initialize variables\n",
        "    current_speaker = None\n",
        "    accumulated_text = \"\"\n",
        "    output_lines = []\n",
        "\n",
        "    # Open and process the input file\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # Check if the line starts with a speaker identifier\n",
        "        if line.startswith((\"Nasim Binesh:\", \"Kasra Ghaharian:\")) or (line[:2].isdigit() and line[2] == ':'):\n",
        "            new_speaker = line.split(':', 1)[0]\n",
        "            # Process the accumulated text if there is a speaker change\n",
        "            if current_speaker and current_speaker != new_speaker and accumulated_text:\n",
        "                # Add an extra newline if the previous speaker was an interviewee\n",
        "                # extra_newline = \"\\n\\n\" if current_speaker.isdigit() else \"\\n\"\n",
        "                extra_newline = \"\\n\"\n",
        "                output_lines.append(f\"{current_speaker}: {accumulated_text.strip()}{extra_newline}\")\n",
        "                accumulated_text = line.split(':', 1)[1].strip()\n",
        "            else:\n",
        "                accumulated_text += \" \" + line.split(':', 1)[1].strip()\n",
        "\n",
        "            current_speaker = new_speaker\n",
        "        else:\n",
        "            # Continue accumulating text for the current speaker\n",
        "            accumulated_text += \" \" + line.strip()\n",
        "\n",
        "        # Add the last accumulated text at the end of file\n",
        "        if i == len(lines) - 1 and accumulated_text:\n",
        "            # extra_newline = \"\\n\\n\" if current_speaker.isdigit() else \"\\n\"\n",
        "            extra_newline = \"\\n\"\n",
        "            output_lines.append(f\"{current_speaker}: {accumulated_text.strip()}{extra_newline}\")\n",
        "\n",
        "    # Write the output lines to the output file\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.writelines(output_lines)\n",
        "\n",
        "def create_quesn_respn_pairs(input_dir, output_dir):\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Process each file in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            input_file_path = os.path.join(input_dir, filename)\n",
        "            output_file_path = os.path.join(output_dir, \"processed_\" + filename)\n",
        "            process_file(input_file_path, output_file_path)\n",
        "            print(f\"Processed {filename}\")\n",
        "\n",
        "# Example usage - Replace these paths with your actual directory paths\n",
        "input_dir = './Dataset'\n",
        "output_dir = './OutputDataset02'\n",
        "\n",
        "# Execute the processing\n",
        "create_quesn_respn_pairs(input_dir, output_dir)\n"
      ],
      "metadata": {
        "id": "AHeHb5Y12pNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bca1cd-bf9a-472a-8b84-73dde0d9b408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 24.txt\n",
            "Processed 09.txt\n",
            "Processed 15.txt\n",
            "Processed 23.txt\n",
            "Processed 13.txt\n",
            "Processed 30.txt\n",
            "Processed 06.txt\n",
            "Processed 25.txt\n",
            "Processed 32.txt\n",
            "Processed 29.txt\n",
            "Processed 21.txt\n",
            "Processed 28.txt\n",
            "Processed 16.txt\n",
            "Processed 08.txt\n",
            "Processed 11.txt\n",
            "Processed 07.txt\n",
            "Processed 05.txt\n",
            "Processed 02.txt\n",
            "Processed 22.txt\n",
            "Processed 33.txt\n",
            "Processed 01.txt\n",
            "Processed 10.txt\n",
            "Processed 19.txt\n",
            "Processed 18.txt\n",
            "Processed 14.txt\n",
            "Processed 26.txt\n",
            "Processed 17.txt\n",
            "Processed 27.txt\n",
            "Processed 04.txt\n",
            "Processed 12.txt\n",
            "Processed 31.txt\n",
            "Processed 03.txt\n",
            "Processed 20.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(line):\n",
        "    \"\"\"Applies NLP cleaning steps to a line of text.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Lowercase conversion and remove punctuation\n",
        "    line = re.sub(r'[^\\w\\s]', '', line.lower())\n",
        "    # Tokenize and remove stop words\n",
        "    tokens = word_tokenize(line)\n",
        "    return ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "def process_transcript(input_file_path):\n",
        "    \"\"\"Processes the transcript to pair questions with responses.\"\"\"\n",
        "    pairs = []\n",
        "    last_question = None\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(':', 1)\n",
        "            if len(parts) == 2:\n",
        "                speaker, text = parts[0], parts[1].strip()\n",
        "                cleaned_text = clean_text(text)\n",
        "\n",
        "                if speaker in [\"Nasim Binesh\", \"Kasra Ghaharian\"]:\n",
        "                    last_question = cleaned_text  # Capture the question\n",
        "                elif speaker.isdigit() and last_question:  # Check if the response follows a question\n",
        "                    pairs.append({'Question': last_question, 'Response': cleaned_text})\n",
        "                    last_question = None  # Reset last_question to avoid pairing with multiple responses\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def clean_transcripts_and_save(input_dir, output_dir):\n",
        "    \"\"\"Cleans transcripts in the directory and saves them as CSV files.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            input_file_path = os.path.join(input_dir, filename)\n",
        "            output_file_path = os.path.join(output_dir, filename.replace('.txt', '_QA.csv'))\n",
        "\n",
        "            pairs = process_transcript(input_file_path)\n",
        "            df = pd.DataFrame(pairs)\n",
        "            df.to_csv(output_file_path, index=False, encoding='utf-8')\n",
        "            print(f\"Processed and cleaned: {filename}\")\n",
        "\n",
        "# Specify the paths to your directories\n",
        "input_dir = './OutputDataset02'  # Update this path\n",
        "output_dir = './Csv'  # Update this path\n",
        "\n",
        "# Execute the processing function\n",
        "clean_transcripts_and_save(input_dir, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw3ivGuoOQIg",
        "outputId": "d78ee1bb-c7e3-41c0-a0c0-6107b97585ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and cleaned: processed_28.txt\n",
            "Processed and cleaned: processed_03.txt\n",
            "Processed and cleaned: processed_02.txt\n",
            "Processed and cleaned: processed_13.txt\n",
            "Processed and cleaned: processed_33.txt\n",
            "Processed and cleaned: processed_08.txt\n",
            "Processed and cleaned: processed_11.txt\n",
            "Processed and cleaned: processed_04.txt\n",
            "Processed and cleaned: processed_10.txt\n",
            "Processed and cleaned: processed_16.txt\n",
            "Processed and cleaned: processed_32.txt\n",
            "Processed and cleaned: processed_22.txt\n",
            "Processed and cleaned: processed_19.txt\n",
            "Processed and cleaned: processed_06.txt\n",
            "Processed and cleaned: processed_05.txt\n",
            "Processed and cleaned: processed_30.txt\n",
            "Processed and cleaned: processed_29.txt\n",
            "Processed and cleaned: processed_26.txt\n",
            "Processed and cleaned: processed_17.txt\n",
            "Processed and cleaned: processed_12.txt\n",
            "Processed and cleaned: processed_20.txt\n",
            "Processed and cleaned: processed_01.txt\n",
            "Processed and cleaned: processed_18.txt\n",
            "Processed and cleaned: processed_27.txt\n",
            "Processed and cleaned: processed_21.txt\n",
            "Processed and cleaned: processed_14.txt\n",
            "Processed and cleaned: processed_31.txt\n",
            "Processed and cleaned: processed_07.txt\n",
            "Processed and cleaned: processed_25.txt\n",
            "Processed and cleaned: processed_09.txt\n",
            "Processed and cleaned: processed_24.txt\n",
            "Processed and cleaned: processed_15.txt\n",
            "Processed and cleaned: processed_23.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import files as colab_files\n",
        "\n",
        "# Creating a zip file of the output_directory\n",
        "zip_file = \"cleaned_transcripts.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files_in_dir in os.walk(output_dir):\n",
        "        for file in files_in_dir:\n",
        "            zipf.write(os.path.join(root, file),\n",
        "                       os.path.relpath(os.path.join(root, file),\n",
        "                                       os.path.join(output_dir, '..')))\n",
        "\n",
        "# Downloading the zip file\n",
        "colab_files.download(zip_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "moKVzPLjFm8K",
        "outputId": "a7e317be-c290-4ea3-e462-e09d8009aa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c15336a1-2409-4181-a6db-9513c083e6a7\", \"cleaned_transcripts.zip\", 442671)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "3VeGK9zv0lkV"
      }
    }
  ]
}